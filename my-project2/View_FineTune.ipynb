{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "CUDA not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "#import torchvision\n",
    "#import torch.nn as nn\n",
    "\n",
    "import os\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "from data_loader import get_train_val_loader, get_test_loader\n",
    "from autoencoder_helpers import *\n",
    "from modules import *\n",
    "from model_testing import *\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "#GPU/CUDA setup\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA enabled!\")\n",
    "    device = torch.device('cuda:0')  # You can specify the index of the CUDA device you want to use\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Specify the index of the GPU(s) you want to use\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "input_height = 28         # MNIST data input shape\n",
    "input_width = input_height\n",
    "  \n",
    "data_folder = 'data'\n",
    "batch_size = 250\n",
    "n_workers = 0\n",
    "random_seed = 0\n",
    "\n",
    "# download MNIST data\n",
    "test_loader = get_test_loader(data_folder, batch_size, shuffle=True, num_workers=n_workers, pin_memory=True)\n",
    "\n",
    "# download MNIST data\n",
    "train_loader, val_loader = get_train_val_loader(data_folder, batch_size, random_seed, augment=False, val_size=0.2,\n",
    "                           shuffle=True, show_sample=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths to saved models\n",
    "paths = {\"B30\": \"saved_model\\mnist_model\\mnist_cae_balanced_clstsep_1500_0.002_250_True_0.0_20_1_1_1_1.0_0.0_30_4_32_1\",\n",
    "         \"S30\": \"saved_model\\mnist_model\\mnist_cae_standard_default_1500_0.002_250_False_0.5_20_1_1_1_0.8_0.2_30_4_32_1\",\n",
    "         \"S15\": \"saved_model\\mnist_model\\mnist_cae_standard_default_1500_0.002_250_False_0.5_20_1_1_1_0.8_0.2_15_4_32_1\",\n",
    "         \"RS30\": \"saved_model\\mnist_model\\mnist_cae_adversarial_standard_default_pdglinf_ce_20_0.3_0.02_True_1500_0.002_250_False_0.5_20_1_1_1_0.8_0.2_1.0_30_4_32_1\",\n",
    "         \"RB30\": \"saved_model\\mnist_model\\mnist_cae_adversarial_balanced_clstsep_pdglinf_ce_20_0.3_0.02_True_800_0.002_250_True_0.0_20_1_1_1_1.0_0.0_1.0_30_4_32_1\",\n",
    "         \"FTB30n\": \"saved_model\\mnist_model\\mnist_cae_FT_30_nothing_pdglinf_ce_20_0.3_0.02_True_20_0.002_250_20_1_1_1_1.0_0.0_1\",\n",
    "         \"FTB30a\": \"saved_model\\mnist_model\\mnist_cae_FT_30_autoencoder_pdglinf_ce_20_0.3_0.02_True_20_0.002_250_20_1_1_1_1.0_0.0_1\",\n",
    "         \"FTB30p\": \"saved_model\\mnist_model\\mnist_cae_FT_30_prototypes_pdglinf_ce_20_0.3_0.02_True_20_0.002_250_20_1_1_1_1.0_0.0_1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\"B30\": \"./saved_model/mnist_model/mnist_cae_balanced_clstsep_1500_0.002_250_True_0.0_20_1_1_1_1.0_0.0_30_4_32_1\",\n",
    "         \"S30\": \"./saved_model/mnist_model/mnist_cae_standard_default_1500_0.002_250_False_0.5_20_1_1_1_0.8_0.2_30_4_32_1\",\n",
    "         \"S15\": \"./saved_model/mnist_model/mnist_cae_standard_default_1500_0.002_250_False_0.5_20_1_1_1_0.8_0.2_15_4_32_1\",\n",
    "         \"RS30\": \"./saved_model/mnist_model/mnist_cae_adversarial_standard_default_pdglinf_ce_20_0.3_0.02_True_1500_0.002_250_False_0.5_20_1_1_1_0.8_0.2_1.0_30_4_32_1\",\n",
    "         \"RB30\": \"./saved_model/mnist_model/mnist_cae_adversarial_balanced_clstsep_pdglinf_ce_20_0.3_0.02_True_800_0.002_250_True_0.0_20_1_1_1_1.0_0.0_1.0_30_4_32_1\",\n",
    "         \"FTB30n\": \"./saved_model/mnist_model/mnist_cae_FT_30_nothing_pdglinf_ce_20_0.3_0.02_True_20_0.002_250_20_1_1_1_1.0_0.0_1\",\n",
    "         \"FTB30a\": \"./saved_model/mnist_model/mnist_cae_FT_30_autoencoder_pdglinf_ce_20_0.3_0.02_True_20_0.002_250_20_1_1_1_1.0_0.0_1\",\n",
    "         \"FTB30p\": \"./saved_model/mnist_model/mnist_cae_FT_30_prototypes_pdglinf_ce_20_0.3_0.02_True_20_0.002_250_20_1_1_1_1.0_0.0_1\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL modules.CAEModel_Balanced was not an allowed global by default. Please use `torch.serialization.add_safe_globals([CAEModel_Balanced])` or the `torch.serialization.safe_globals([CAEModel_Balanced])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory_path, model_file)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the model and fit PCA to the test data\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1464\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1468\u001b[0m                 )\n\u001b[1;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1472\u001b[0m             opened_zipfile,\n\u001b[1;32m   1473\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1477\u001b[0m         )\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL modules.CAEModel_Balanced was not an allowed global by default. Please use `torch.serialization.add_safe_globals([CAEModel_Balanced])` or the `torch.serialization.safe_globals([CAEModel_Balanced])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# Path to the directory containing model files\n",
    "directory_path = paths[\"FTB30n\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# List of specific model files to process\n",
    "specific_models = [\"mnist_cae_adv00000.pth\", \"mnist_cae_adv00010.pth\", \"mnist_cae_adv00020.pth\"]\n",
    "\n",
    "# Plot and save the images for the specific models\n",
    "for idx, model_file in enumerate(specific_models):\n",
    "    model_path = os.path.join(directory_path, model_file)\n",
    "    \n",
    "    # Load the model and fit PCA to the test data\n",
    "    model = torch.load(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    reduced_test_data, test_labels, pca = get_encoded_test_data_and_fit_pca(test_loader, model, device)\n",
    "    \n",
    "    # Project the prototypes using the PCA fitted on test data\n",
    "    reduced_prototypes, prototype_imgs = get_prototype_projection(model_path, device, pca)\n",
    "    \n",
    "    # Calculate the x and y limits for the current model's data\n",
    "    xlim = (min(reduced_test_data[:, 0].min(), reduced_prototypes[:, 0].min()), max(reduced_test_data[:, 0].max(), reduced_prototypes[:, 0].max()))\n",
    "    ylim = (min(reduced_test_data[:, 1].min(), reduced_prototypes[:, 1].min()), max(reduced_test_data[:, 1].max(), reduced_prototypes[:, 1].max()))\n",
    "    \n",
    "    if idx == 0:\n",
    "        title = '2D Projection of Prototypes - Before'\n",
    "    elif idx == len(specific_models) - 1:\n",
    "        title = '2D Projection of Prototypes - After'\n",
    "    else:\n",
    "        title = '2D Projection of Prototypes - Middle'\n",
    "    \n",
    "    save_path = os.path.join(directory_path, f'{model_file}_projection.png')\n",
    "    plot_prototype_projection_with_data(reduced_prototypes, prototype_imgs, reduced_test_data, test_labels, title, xlim, ylim, save_path)\n",
    "    \n",
    "    image = plt.imread(save_path)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
