# PrototypesVulnerabilities
Master Final Thesis - Research into the security and robustness of prototype-based explanation methods within XAI.

## Introduction

In recent years, explainable artificial intelligence (XAI) has gained significant attention as a critical component for building trust and transparency in machine learning models. Among the various approaches to XAI, prototype-based explanation methods have emerged as a promising technique, offering intuitive and human-interpretable insights into model decisions. These methods rely on identifying representative prototypes within the dataset to explain the behavior of complex models, such as deep neural networks. However, as the adoption of these methods grows, so does the need to ensure their security and robustness, particularly in the face of adversarial attacks.

Adversarial attacks pose a serious threat to machine learning systems, where malicious actors manipulate input data to deceive models into making incorrect predictions. While much research has focused on the robustness of models themselves, the vulnerability of explanation methods, including prototype-based approaches, remains an underexplored area. This project aims to investigate the security and robustness of prototype-based explanation methods under adversarial conditions. Specifically, we explore how these methods behave when subjected to adversarial perturbations, whether they can be exploited to mislead interpretations, and how to enhance their resilience against such attacks.

By addressing these challenges, this project seeks to contribute to the development of more reliable and trustworthy XAI systems, ensuring that prototype-based explanations remain accurate and meaningful even in adversarial environments. Through a combination of theoretical analysis, empirical experiments, and practical recommendations, we aim to provide insights and tools that strengthen the security of explainable AI methods in real-world applications.


